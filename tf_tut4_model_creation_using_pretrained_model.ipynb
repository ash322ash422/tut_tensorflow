{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06d78197-b6ba-4139-bb63-2f75db8141b3",
   "metadata": {},
   "source": [
    "## Using Pretrained Models\n",
    "TensorFlow provides a wide range of pretrained models in tf.keras.applications. These models are pre-trained on large datasets (like ImageNet) and can be used directly for transfer learning.\n",
    "\n",
    "The VGG16 model you loaded was initialized without the top classification layers (include_top=False), which means it does not provide predictions in terms of human-readable class labels (like \"elephant\" or \"dog\"). Instead, it outputs feature maps from the convolutional layers.\n",
    "\n",
    "Example (Using a Pretrained Model for Image Classification): (Took 17 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da0398fd-c07e-4774-9fdc-bfe19e28de4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 0us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 675ms/step\n",
      "Predictions: [[[[-0.         -0.         -0.         ... -0.         -0.\n",
      "    -0.        ]\n",
      "   [-0.         -0.         -0.         ... -0.         -0.\n",
      "    -0.        ]\n",
      "   [-0.         -0.         19.05331    ... -0.         -0.\n",
      "    -0.        ]\n",
      "   ...\n",
      "   [-0.         -0.         38.652523   ... -0.         -0.\n",
      "    -0.        ]\n",
      "   [-0.         -0.          7.3346276  ... -0.          2.0215232\n",
      "    -0.        ]\n",
      "   [-0.         -0.         -0.         ... -0.          4.0603657\n",
      "    -0.        ]]\n",
      "\n",
      "  [[-0.         -0.         -0.         ... -0.         -0.\n",
      "    -0.        ]\n",
      "   [-0.         -0.         -0.         ... -0.         -0.\n",
      "    -0.        ]\n",
      "   [ 2.7255654  -0.         22.054188   ... -0.         -0.\n",
      "    18.784767  ]\n",
      "   ...\n",
      "   [26.836723   -0.         44.098248   ... -0.         -0.\n",
      "    -0.        ]\n",
      "   [-0.         -0.         28.540737   ... -0.         -0.\n",
      "    -0.        ]\n",
      "   [-0.         -0.         -0.         ... -0.         -0.\n",
      "     0.46292737]]\n",
      "\n",
      "  [[-0.         -0.         -0.         ... -0.          1.4470661\n",
      "    -0.        ]\n",
      "   [-0.         -0.          2.9581008  ... -0.         -0.\n",
      "    -0.        ]\n",
      "   [-0.         -0.          4.131012   ... -0.         -0.\n",
      "    16.213247  ]\n",
      "   ...\n",
      "   [35.149517   -0.         -0.         ... 47.23402    -0.\n",
      "    18.974838  ]\n",
      "   [-0.         -0.         -0.         ... -0.         -0.\n",
      "     5.774252  ]\n",
      "   [-0.         -0.         -0.         ... -0.         -0.\n",
      "     5.5787373 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.         -0.         -0.         ... -0.         -0.\n",
      "    -0.        ]\n",
      "   [-0.         -0.         -0.         ... -0.         -0.\n",
      "    -0.        ]\n",
      "   [-0.         -0.         -0.         ... -0.         -0.\n",
      "    -0.        ]\n",
      "   ...\n",
      "   [-0.         -0.         10.079561   ... -0.         -0.\n",
      "    -0.        ]\n",
      "   [-0.         -0.         -0.         ... -0.         -0.\n",
      "    -0.        ]\n",
      "   [-0.         -0.         -0.         ... -0.         -0.\n",
      "    -0.        ]]\n",
      "\n",
      "  [[-0.         -0.         -0.         ... -0.         -0.\n",
      "    -0.        ]\n",
      "   [-0.         -0.         -0.         ... -0.         -0.\n",
      "    -0.        ]\n",
      "   [-0.         -0.         -0.         ... 12.396514   -0.\n",
      "    -0.        ]\n",
      "   ...\n",
      "   [-0.         -0.         23.561779   ... -0.         -0.\n",
      "    -0.        ]\n",
      "   [-0.         -0.          5.734057   ... -0.         -0.\n",
      "    -0.        ]\n",
      "   [-0.         -0.         -0.         ... -0.         -0.\n",
      "    -0.        ]]\n",
      "\n",
      "  [[-0.         -0.         -0.         ... -0.         -0.\n",
      "    -0.        ]\n",
      "   [-0.         -0.         -0.         ... -0.         -0.\n",
      "    -0.        ]\n",
      "   [-0.         -0.         -0.         ... -0.         -0.\n",
      "    -0.        ]\n",
      "   ...\n",
      "   [-0.         -0.         -0.         ... -0.         -0.\n",
      "    -0.        ]\n",
      "   [-0.         -0.         -0.         ... -0.         -0.\n",
      "    -0.        ]\n",
      "   [-0.         -0.         -0.         ... -0.         -0.\n",
      "    -0.        ]]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "# Load a pretrained VGG16 model (without top layers)\n",
    "model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "# Load an image\n",
    "img_path = 'elephant.jpg'  # Change this to a valid image file path\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "# Preprocess the image for VGG16\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "img_array = tf.keras.applications.vgg16.preprocess_input(img_array)\n",
    "\n",
    "# Get model predictions\n",
    "predictions = model.predict(img_array)\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642d76e6-4b4f-4c1a-9ac1-773aa3053b4d",
   "metadata": {},
   "source": [
    "# Covert above output to Human readable form\n",
    "The VGG16 model you loaded was initialized without the top classification layers (include_top=False), which means it does not provide predictions in terms of human-readable class labels (like \"elephant\" or \"dog\"). Instead, it outputs feature maps from the convolutional layers.\n",
    "\n",
    "To get predictions as human-readable class labels, you need to use the full VGG16 model with the top layers included. Here's how you can do that:(Took 3 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ace273c6-36e6-477b-a8c6-4d224cf6ba26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "\u001b[1m553467096/553467096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 0us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 686ms/step\n",
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
      "\u001b[1m35363/35363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Predictions (Top 3):\n",
      "1: tusker (0.49)\n",
      "2: African_elephant (0.45)\n",
      "3: Indian_elephant (0.06)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import decode_predictions\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "# Load a pretrained VGG16 model (with top layers)\n",
    "model = VGG16(weights='imagenet')\n",
    "\n",
    "# Load an image\n",
    "img_path = 'elephant.jpg'  # Change this to a valid image file path\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "# Preprocess the image for VGG16\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "img_array = tf.keras.applications.vgg16.preprocess_input(img_array)\n",
    "\n",
    "# Get model predictions\n",
    "predictions = model.predict(img_array)\n",
    "\n",
    "# Decode the predictions into human-readable labels\n",
    "decoded_predictions = decode_predictions(predictions, top=3)  # Get top 3 predictions\n",
    "print(\"Predictions (Top 3):\")\n",
    "for i, (imagenet_id, label, score) in enumerate(decoded_predictions[0]):\n",
    "    print(f\"{i + 1}: {label} ({score:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5adab44-83fc-4648-a65e-03f5b03d2c19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
