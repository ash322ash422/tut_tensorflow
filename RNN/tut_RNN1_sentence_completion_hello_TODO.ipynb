{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfab82eb-d6e6-4024-a2a8-4f13cc6170a0",
   "metadata": {},
   "source": [
    "## Toy example1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccf4a778-fdce-483f-96d3-1edb827a90c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.4917757511138916\n",
      "Generated: heeel\n",
      "Epoch 100, Loss: 0.6213928461074829\n",
      "Generated: elloh\n",
      "Epoch 200, Loss: 0.2752682566642761\n",
      "Generated: elloh\n",
      "Epoch 300, Loss: 0.14365044236183167\n",
      "Generated: elloh\n",
      "Epoch 400, Loss: 0.08586446195840836\n",
      "Generated: elloh\n",
      "Epoch 500, Loss: 0.056807905435562134\n",
      "Generated: elloh\n",
      "Epoch 600, Loss: 0.040397919714450836\n",
      "Generated: elloh\n",
      "Epoch 700, Loss: 0.030260393396019936\n",
      "Generated: elloh\n",
      "Epoch 800, Loss: 0.023553723469376564\n",
      "Generated: elloh\n",
      "Epoch 900, Loss: 0.01887591928243637\n",
      "Generated: elloh\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "data = \"hello\"\n",
    "chars = list(set(data))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Prepare inputs\n",
    "input_seq = [char_to_ix[ch] for ch in data]\n",
    "target_seq = [char_to_ix[ch] for ch in data[1:] + data[0]]\n",
    "\n",
    "input_seq = tf.one_hot(input_seq, vocab_size)  # One-hot encoding\n",
    "input_seq = tf.expand_dims(input_seq, 0)        # Add batch dimension\n",
    "target_seq = tf.expand_dims(target_seq, 0)\n",
    "\n",
    "# Build model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(10, return_sequences=True),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "])\n",
    "\n",
    "# Compile\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              optimizer='adam')\n",
    "\n",
    "# Train\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model(input_seq)\n",
    "        loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(\n",
    "            target_seq, output, from_logits=True))\n",
    "    \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.numpy()}')\n",
    "\n",
    "        # Sampling\n",
    "        prediction = tf.argmax(output, axis=-1)\n",
    "        sampled_text = ''.join(ix_to_char[ix.numpy()] for ix in prediction[0])\n",
    "        print(f'Generated: {sampled_text}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f6199a-f2d8-4385-a2cc-09c1ab35fb73",
   "metadata": {},
   "source": [
    "## Toy example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "940c1557-4a91-4c2a-91b4-6df1e2d75a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded text length: 148080 characters\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK 11 ***\n",
      "\n",
      "[Illustration]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Alices Adventures in Wond\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "\n",
    "# Step 1: Download the text\n",
    "url = 'https://www.gutenberg.org/files/11/11-0.txt'\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "print(f\"Downloaded text length: {len(text)} characters\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02d1dd00-647a-4c42-a07e-cf5dadd6df0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** START OF THE PROJECT GUTENBERG EBOOK 11 ***\n",
      "\n",
      "[Illustration]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Alices Adventures in Wonderland\n",
      "\n",
      "by Lewis Carroll\n",
      "\n",
      "THE MILLENNIUM FULCRUM EDITION 3.0\n",
      "\n",
      "Contents\n",
      "\n",
      " CHAPTER I.     Down the Rabbit-Hole\n",
      " CHAPTER II.    The Pool of Tears\n",
      " CHAPTER III.   A Caucus-Race and a Long Tale\n",
      " CHAPTER IV.    The Rabbit Sends in a Little Bill\n",
      " CHAPTER V.     Advice from a Caterpillar\n",
      " CHAPTER VI.    Pig and Pepper\n",
      " CHAPTER VII.   A Mad Tea-Party\n",
      " CHAPTER VIII.  The Queens Croquet-Ground\n",
      " CHAPTER IX.    The Mock Turtles Story\n",
      " CHAPTER X.     The Lobster Quadrille\n",
      " CHAPTER XI.    Who Stole the Tarts?\n",
      " CHAPTER XII.   Alices Evidence\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CHAPTER I.\n",
      "Down the Rabbit-Hole\n",
      "\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothing to do: once or twice she had peeped into\n",
      "the book her sister was reading, but it had no pictures or\n",
      "conversations in it, and what is the use of a book, thought Alice\n",
      "without pictures or conversations?\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Preprocess the text\n",
    "# Only keep ASCII characters to avoid weird symbols\n",
    "text = ''.join(c for c in text if ord(c) < 128)\n",
    "print(text[:1000])\n",
    "\n",
    "# text = \"This is GeeksforGeeks a software training institute\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb4304ca-a69c-4d3e-811b-36b3c8b88ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "char_to_index = {char: i for i, char in enumerate(chars)}\n",
    "index_to_char = {i: char for i, char in enumerate(chars)}\n",
    "\n",
    "seq_length = 3\n",
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "for i in range(len(text) - seq_length):\n",
    "    seq = text[i:i + seq_length]\n",
    "    label = text[i + seq_length]\n",
    "    sequences.append([char_to_index[char] for char in seq])\n",
    "    labels.append(char_to_index[label])\n",
    "\n",
    "X = np.array(sequences)\n",
    "y = np.array(labels)\n",
    "\n",
    "X_one_hot = tf.one_hot(X, len(chars))\n",
    "y_one_hot = tf.one_hot(y, len(chars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36597fb5-a985-4abb-8676-a3e5b8adbed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - accuracy: 0.3130 - loss: 2.6076\n",
      "Epoch 2/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.4361 - loss: 2.0186\n",
      "Epoch 3/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.4570 - loss: 1.9229\n",
      "Epoch 4/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.4667 - loss: 1.8753\n",
      "Epoch 5/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.4769 - loss: 1.8392\n",
      "Epoch 6/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.4840 - loss: 1.8126\n",
      "Epoch 7/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.4918 - loss: 1.7854\n",
      "Epoch 8/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.4913 - loss: 1.7734\n",
      "Epoch 9/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.4963 - loss: 1.7556\n",
      "Epoch 10/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - accuracy: 0.5001 - loss: 1.7417\n",
      "Epoch 11/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5009 - loss: 1.7361\n",
      "Epoch 12/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5064 - loss: 1.7174\n",
      "Epoch 13/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.5080 - loss: 1.7100\n",
      "Epoch 14/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5102 - loss: 1.7006\n",
      "Epoch 15/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5097 - loss: 1.7012\n",
      "Epoch 16/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.5130 - loss: 1.6862\n",
      "Epoch 17/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5124 - loss: 1.6865\n",
      "Epoch 18/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - accuracy: 0.5149 - loss: 1.6811\n",
      "Epoch 19/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5126 - loss: 1.6842\n",
      "Epoch 20/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - accuracy: 0.5156 - loss: 1.6753\n",
      "Epoch 21/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5146 - loss: 1.6709\n",
      "Epoch 22/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - accuracy: 0.5154 - loss: 1.6719\n",
      "Epoch 23/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - accuracy: 0.5170 - loss: 1.6634\n",
      "Epoch 24/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - accuracy: 0.5197 - loss: 1.6593\n",
      "Epoch 25/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - accuracy: 0.5177 - loss: 1.6616\n",
      "Epoch 26/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - accuracy: 0.5200 - loss: 1.6567\n",
      "Epoch 27/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5180 - loss: 1.6536\n",
      "Epoch 28/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - accuracy: 0.5204 - loss: 1.6465\n",
      "Epoch 29/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5221 - loss: 1.6480\n",
      "Epoch 30/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5226 - loss: 1.6409\n",
      "Epoch 31/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5199 - loss: 1.6434\n",
      "Epoch 32/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - accuracy: 0.5212 - loss: 1.6441\n",
      "Epoch 33/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.5200 - loss: 1.6429\n",
      "Epoch 34/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.5228 - loss: 1.6389\n",
      "Epoch 35/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5199 - loss: 1.6408\n",
      "Epoch 36/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - accuracy: 0.5227 - loss: 1.6329\n",
      "Epoch 37/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.5212 - loss: 1.6411\n",
      "Epoch 38/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5235 - loss: 1.6338\n",
      "Epoch 39/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5216 - loss: 1.6341\n",
      "Epoch 40/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5198 - loss: 1.6481\n",
      "Epoch 41/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5223 - loss: 1.6310\n",
      "Epoch 42/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5229 - loss: 1.6331\n",
      "Epoch 43/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - accuracy: 0.5217 - loss: 1.6318\n",
      "Epoch 44/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.5230 - loss: 1.6308\n",
      "Epoch 45/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.5249 - loss: 1.6181\n",
      "Epoch 46/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - accuracy: 0.5240 - loss: 1.6301\n",
      "Epoch 47/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.5230 - loss: 1.6269\n",
      "Epoch 48/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - accuracy: 0.5246 - loss: 1.6255\n",
      "Epoch 49/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.5242 - loss: 1.6235\n",
      "Epoch 50/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - accuracy: 0.5242 - loss: 1.6227\n",
      "Epoch 51/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - accuracy: 0.5223 - loss: 1.6258\n",
      "Epoch 52/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - accuracy: 0.5240 - loss: 1.6268\n",
      "Epoch 53/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - accuracy: 0.5259 - loss: 1.6184\n",
      "Epoch 54/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - accuracy: 0.5248 - loss: 1.6239\n",
      "Epoch 55/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - accuracy: 0.5221 - loss: 1.6230\n",
      "Epoch 56/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - accuracy: 0.5237 - loss: 1.6216\n",
      "Epoch 57/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - accuracy: 0.5242 - loss: 1.6235\n",
      "Epoch 58/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5256 - loss: 1.6148\n",
      "Epoch 59/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5267 - loss: 1.6161\n",
      "Epoch 60/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5259 - loss: 1.6188\n",
      "Epoch 61/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5261 - loss: 1.6132\n",
      "Epoch 62/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5248 - loss: 1.6143\n",
      "Epoch 63/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - accuracy: 0.5258 - loss: 1.6149\n",
      "Epoch 64/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5268 - loss: 1.6135\n",
      "Epoch 65/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5268 - loss: 1.6104\n",
      "Epoch 66/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5296 - loss: 1.6025\n",
      "Epoch 67/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.5265 - loss: 1.6093\n",
      "Epoch 68/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5262 - loss: 1.6155\n",
      "Epoch 69/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5273 - loss: 1.6109\n",
      "Epoch 70/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - accuracy: 0.5252 - loss: 1.6157\n",
      "Epoch 71/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.5286 - loss: 1.6020\n",
      "Epoch 72/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.5282 - loss: 1.6055\n",
      "Epoch 73/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - accuracy: 0.5256 - loss: 1.6137\n",
      "Epoch 74/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - accuracy: 0.5289 - loss: 1.6094\n",
      "Epoch 75/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - accuracy: 0.5267 - loss: 1.6062\n",
      "Epoch 76/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3ms/step - accuracy: 0.5275 - loss: 1.6111\n",
      "Epoch 77/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.5286 - loss: 1.6034\n",
      "Epoch 78/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.5293 - loss: 1.6017\n",
      "Epoch 79/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - accuracy: 0.5278 - loss: 1.6076\n",
      "Epoch 80/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5277 - loss: 1.6066\n",
      "Epoch 81/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5249 - loss: 1.6091\n",
      "Epoch 82/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - accuracy: 0.5258 - loss: 1.6084\n",
      "Epoch 83/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - accuracy: 0.5236 - loss: 1.6121\n",
      "Epoch 84/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - accuracy: 0.5275 - loss: 1.6048\n",
      "Epoch 85/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.5276 - loss: 1.6063\n",
      "Epoch 86/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.5269 - loss: 1.6074\n",
      "Epoch 87/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5267 - loss: 1.6027\n",
      "Epoch 88/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - accuracy: 0.5275 - loss: 1.6053\n",
      "Epoch 89/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.5255 - loss: 1.6049\n",
      "Epoch 90/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 3ms/step - accuracy: 0.5250 - loss: 1.6045\n",
      "Epoch 91/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - accuracy: 0.5287 - loss: 1.6016\n",
      "Epoch 92/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.5292 - loss: 1.5961\n",
      "Epoch 93/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5264 - loss: 1.6063\n",
      "Epoch 94/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.5263 - loss: 1.6028\n",
      "Epoch 95/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.5285 - loss: 1.6009\n",
      "Epoch 96/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5279 - loss: 1.6043\n",
      "Epoch 97/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - accuracy: 0.5301 - loss: 1.5948\n",
      "Epoch 98/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - accuracy: 0.5306 - loss: 1.5969\n",
      "Epoch 99/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - accuracy: 0.5298 - loss: 1.6012\n",
      "Epoch 100/100\n",
      "\u001b[1m4526/4526\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - accuracy: 0.5289 - loss: 1.5975\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(50, input_shape=(seq_length, len(chars)), activation='relu'))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "import time\n",
    "class TimeHistory(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.times = []\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_time_start = time.time()\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.times.append(time.time() - self.epoch_time_start)\n",
    "\n",
    "# Create callback\n",
    "time_callback = TimeHistory()\n",
    "\n",
    "# Train model with callback\n",
    "history = model.fit(X_one_hot, y_one_hot, epochs=EPOCHS, callbacks=[time_callback])\n",
    "\n",
    "# model.fit(X_one_hot, y_one_hot, epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a8b7399-f526-4bcc-b48e-669b885a825c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time per epoch (in seconds): [12.50995397567749, 11.113578081130981, 10.49731159210205, 10.757098197937012, 10.760200262069702, 10.203116178512573, 10.135388374328613, 10.34597659111023, 11.045531749725342, 20.607378005981445, 10.784028053283691, 11.100220918655396, 11.408073425292969, 11.050654888153076, 11.265387058258057, 11.448534965515137, 11.0776207447052, 20.610201358795166, 11.087640047073364, 20.63566017150879, 11.101003885269165, 20.61864686012268, 20.759530544281006, 20.504194736480713, 20.5325186252594, 20.66722846031189, 11.168475866317749, 20.42527484893799, 10.750025510787964, 10.51625394821167, 11.300658464431763, 19.580969095230103, 10.353837251663208, 10.241546630859375, 11.282840728759766, 20.78786277770996, 20.44586491584778, 11.318994283676147, 11.292000770568848, 11.344514846801758, 11.352830410003662, 11.358193635940552, 20.80751323699951, 20.490063190460205, 11.362921714782715, 20.65556240081787, 11.4129958152771, 20.71203875541687, 20.423572778701782, 20.76060175895691, 20.602639198303223, 11.557569026947021, 20.56265950202942, 20.721208333969116, 11.809251070022583, 11.58976125717163, 12.254001379013062, 10.506027936935425, 11.281729221343994, 10.869980573654175, 10.546003341674805, 11.026412963867188, 12.261946439743042, 10.764517307281494, 10.883007287979126, 11.324036121368408, 10.454091548919678, 11.297998666763306, 11.334517240524292, 20.741915702819824, 20.409059524536133, 11.419999122619629, 21.24520468711853, 12.111587047576904, 20.658311128616333, 21.897488117218018, 11.384145259857178, 10.484590291976929, 11.986964464187622, 10.600637435913086, 10.56929636001587, 12.284470558166504, 12.424304008483887, 11.514631509780884, 11.389798641204834, 11.392456769943237, 11.06366777420044, 12.32910704612732, 12.878241300582886, 14.533828258514404, 11.59489631652832, 13.492988109588623, 11.079198598861694, 12.666592836380005, 11.40014100074768, 11.303468465805054, 12.150254726409912, 11.662706136703491, 11.859493732452393, 11.793184041976929]\n",
      "Total training time: 22.95 minutes\n"
     ]
    }
   ],
   "source": [
    "# After training, you can see time per epoch\n",
    "print(\"\\nTime per epoch (in seconds):\", time_callback.times)\n",
    "\n",
    "# Total training time in minutes\n",
    "total_time_min = sum(time_callback.times) / 60\n",
    "print(f\"Total training time: {total_time_min:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71438a4f-46d7-41ff-a78b-43021fab11d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Generated Text:\n",
      "Alice was the was the was the was the was the was the w\n"
     ]
    }
   ],
   "source": [
    "start_seq = \"Alice\"\n",
    "generated_text = start_seq\n",
    "\n",
    "for i in range(50):\n",
    "    x = np.array([[char_to_index[char] for char in generated_text[-seq_length:]]])\n",
    "    x_one_hot = tf.one_hot(x, len(chars))\n",
    "    prediction = model.predict(x_one_hot)\n",
    "    next_index = np.argmax(prediction)\n",
    "    next_char = index_to_char[next_index]\n",
    "    generated_text += next_char\n",
    "\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0552f25-e3b3-47ab-b090-ca2430a3d963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
