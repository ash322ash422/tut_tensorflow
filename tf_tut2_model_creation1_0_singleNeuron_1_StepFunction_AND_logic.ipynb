{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Perceptron with 1 neuron for AND gate using step activation function\n",
        "- Here we are manually set up the weights and bias.\n",
        "- Here we are not training the model, because wts and bias are already predefined"
      ],
      "metadata": {
        "id": "3UZ3KM1H5Sb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "\n",
        "# Reproducibility\n",
        "def set_seed(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "set_seed()\n",
        "\n",
        "# Define AND gate inputs and outputs\n",
        "X = np.array([[0, 0], # out -> 0\n",
        "              [0, 1], # out -> 0\n",
        "              [1, 0], # out -> 0\n",
        "              [1, 1]  # out -> 1\n",
        "              ], dtype=np.float32)\n",
        "\n",
        "# We do not need y because we are not trianing\n",
        "\n",
        "# Custom step activation function\n",
        "def step_function(x):\n",
        "    return tf.cast(tf.greater_equal(x, 0), tf.float32)\n",
        "\n",
        "# Build model with 1 neuron and custom activation\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(1, activation=step_function, input_shape=(2,))\n",
        "])\n",
        "\n",
        "# Use Mean Squared Error since output is binary and step is non-differentiable\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),\n",
        "              loss='mse')\n",
        "\n",
        "# Training may not work properly due to non-differentiable step function,\n",
        "# so we'll manually set weights to make AND work\n",
        "# wt for input1 = 1.0\n",
        "# wt for input2 = 1.0\n",
        "# bias = -1.5\n",
        "model.layers[0].set_weights([np.array([[1.0], [1.0]]), np.array([-1.5])])\n",
        "\n",
        "predictions = model.predict(X) # Predict\n",
        "\n",
        "# Output\n",
        "print(\"\\nAND Gate with Step Function:\")\n",
        "for i, pred in enumerate(predictions):\n",
        "    print(f\"Input: {X[i]} → Output: {pred[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqL8JyNX6G9V",
        "outputId": "1b7300a4-b25c-495a-a463-96137999f8c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
            "\n",
            "AND Gate with Step Function:\n",
            "Input: [0. 0.] → Output: 0.0\n",
            "Input: [0. 1.] → Output: 0.0\n",
            "Input: [1. 0.] → Output: 0.0\n",
            "Input: [1. 1.] → Output: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Now lets create a perceptron with 1 neuron with step function that can simulate output\n",
        " ```\n",
        "Input  -> Output\n",
        "[0. 0.] → 0\n",
        "[0. 1.] → 1\n",
        "[1. 0.] → 0\n",
        "[1. 1.] → 1\n",
        "```\n",
        "- Here we manually set the weights and bias\n",
        "- Here we are not training the model, because wts and bias are already predefined"
      ],
      "metadata": {
        "id": "wuADJwcP_EHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "\n",
        "# Reproducibility\n",
        "def set_seed(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "set_seed()\n",
        "\n",
        "# Define AND gate inputs and outputs\n",
        "X = np.array([[0, 0], # out -> 0\n",
        "              [0, 1], # out -> 1\n",
        "              [1, 0], # out -> 0\n",
        "              [1, 1]  # out -> 1\n",
        "              ], dtype=np.float32)\n",
        "\n",
        "# We do not need y because we are not trianing\n",
        "\n",
        "# Custom step activation function\n",
        "def step_function(x):\n",
        "    return tf.cast(tf.greater_equal(x, 0), tf.float32)\n",
        "\n",
        "# Build model with 1 neuron and custom activation\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(1, activation=step_function, input_shape=(2,))\n",
        "])\n",
        "\n",
        "# Use Mean Squared Error since output is binary and step is non-differentiable\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),\n",
        "              loss='mse')\n",
        "\n",
        "# Training may not work properly due to non-differentiable step function,\n",
        "# so we'll manually set weights to make AND work\n",
        "# wt for input1 = 0.2\n",
        "# wt for input2 = 0.5\n",
        "# bias = -0.5\n",
        "model.layers[0].set_weights([np.array([[0.2], [0.5]]), np.array([-0.5])])\n",
        "\n",
        "predictions = model.predict(X) # Predict\n",
        "\n",
        "# Output\n",
        "print(\"\\n Step Function:\")\n",
        "for i, pred in enumerate(predictions):\n",
        "    print(f\"Input: {X[i]} → Output: {pred[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-tkGrkR-S8E",
        "outputId": "c32746d9-7f99-49c9-f9c1-346a0f7aae8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "\n",
            " Step Function:\n",
            "Input: [0. 0.] → Output: 0.0\n",
            "Input: [0. 1.] → Output: 1.0\n",
            "Input: [1. 0.] → Output: 0.0\n",
            "Input: [1. 1.] → Output: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Lets make above little more realistic\n",
        "- Here we manually set the weights and bias\n",
        "- Here we are not training the model, because wts and bias are already predefined\n",
        "\n",
        "We'll treat:\n",
        "\n",
        "x1 = temperature (in °C)\n",
        "\n",
        "x2 = pressure (in kPa)\n",
        "\n",
        "We want the neuron to fire (output = 1) only if:\n",
        "\n",
        "temperature > 50, and\n",
        "\n",
        "pressure > 100\n",
        "\n",
        "Let's derive the weights and bias so that the neuron fires (output = 1) only when both:\n",
        "\n",
        "temperature > 50\n",
        "\n",
        "pressure > 100\n",
        "\n",
        "This is an AND-like condition over two continuous variables.\n",
        "\n",
        "A perceptron with a step activation computes:\n",
        "- output = 1 if w1 * temp + w2 * pressure + b >= 0\n",
        "- output = 0 , otherwise\n",
        "\n",
        "i.e. we want following output using step function:\n",
        "- For values just above (50, 100) → output = 1\n",
        "- For values at or below (50, 100) → output = 0\n",
        "\n",
        "###Strategy:###\n",
        "Choose simple weights: w1 = 1, w2 = 1.\n",
        "\n",
        "Then solve: temp + pressure + b >= 0\n",
        "\n",
        "We want the neuron to fire at temp = 51 and pressure = 101, but not at or below 50, 100.\n",
        "\n",
        "So:\n",
        "51+101+b≥0⇒152+b≥0⇒b≥−152\n",
        "\n",
        "and\n",
        "\n",
        "50+100+b<0⇒150+b<0⇒b<−150\n",
        "\n",
        "Thus, pick:\n",
        "\n",
        "w1=1, w2=1, b=−151\n",
        "​\n"
      ],
      "metadata": {
        "id": "LXW1Vp06HqfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Sample realistic inputs\n",
        "# Columns: [temperature, pressure]\n",
        "# Inputs\n",
        "X = np.array([\n",
        "    [50, 100],   # exactly at threshold → 0\n",
        "    [53, 105],   # both just above → 1\n",
        "    [60, 120],   # both well above → 1\n",
        "    [45, 90],    # both below → 0\n",
        "    [40, 90],     # both below → 0\n",
        "\n",
        "], dtype=np.float32)\n",
        "\n",
        "# We do not need y because we are not training\n",
        "\n",
        "# Custom step activation function\n",
        "def step_function(x):\n",
        "    return tf.cast(tf.greater_equal(x, 0), tf.float32)\n",
        "\n",
        "# Define 1-neuron model with step activation\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(1, activation=step_function, input_shape=(2,))\n",
        "])\n",
        "\n",
        "# Manually set weights and bias to match threshold logic:\n",
        "weights = np.array([[1.0], [1.0]])  # temp & pressure weights\n",
        "bias = np.array([-151.0])           # threshold offset\n",
        "model.layers[0].set_weights([weights, bias])\n",
        "\n",
        "\n",
        "# Predict\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# Output\n",
        "print(\"Realistic Inputs - AND-like Step Function Output:\")\n",
        "for i, pred in enumerate(predictions):\n",
        "    print(f\"Input: {X[i]} → Output: {int(pred[0])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rhzFf3NFyXO",
        "outputId": "ca4189fe-9d51-47cc-fc9a-6262d75b25ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "Realistic Inputs - AND-like Step Function Output:\n",
            "Input: [ 50. 100.] → Output: 0\n",
            "Input: [ 53. 105.] → Output: 1\n",
            "Input: [ 60. 120.] → Output: 1\n",
            "Input: [45. 90.] → Output: 0\n",
            "Input: [40. 90.] → Output: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## But this is not what we want. We want to train our model from X and y , and figure out weights and bias from the training data.\n",
        "\n",
        "Switching to sigmoid allows your single-neuron perceptron to:\n",
        "- Be trained automatically using a loss function and backpropagation\n",
        "- Produce probabilistic outputs\n",
        "- Generalize better on real-valued input spaces"
      ],
      "metadata": {
        "id": "GUxx1U37dsJq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lets Train our 1 neuron perceptron\n",
        "### 4) 1 Neuron perceptron for AND gate using sigmoid activation function"
      ],
      "metadata": {
        "id": "XKMuC4iy9S3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2000 epoch took 1 minute\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "\n",
        "# For consistent results\n",
        "def set_seed(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "set_seed()\n",
        "\n",
        "# AND gate dataset\n",
        "X = np.array([[0, 0],\n",
        "              [0, 1],\n",
        "              [1, 0],\n",
        "              [1, 1]], dtype=np.float32)\n",
        "\n",
        "y = np.array([[0],\n",
        "              [0],\n",
        "              [0],\n",
        "              [1]], dtype=np.float32)\n",
        "\n",
        "# Define 1-neuron model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(2,))\n",
        "])\n",
        "\n",
        "# Compile with better optimizer\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.5),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=2000, verbose=0)\n",
        "\n",
        "# Predict\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# Output\n",
        "print(\"\\nAND Gate Predictions:\")\n",
        "for i, pred in enumerate(predictions):\n",
        "    print(f\"Input: {X[i]} → Predicted: {pred[0]:.4f}, Rounded: {round(pred[0])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSPCCiusvMLS",
        "outputId": "68fa565e-5d5b-4913-ca12-81382cec0d03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "\n",
            "AND Gate Predictions:\n",
            "Input: [0. 0.] → Predicted: 0.0000, Rounded: 0\n",
            "Input: [0. 1.] → Predicted: 0.0201, Rounded: 0\n",
            "Input: [1. 0.] → Predicted: 0.0201, Rounded: 0\n",
            "Input: [1. 1.] → Predicted: 0.9718, Rounded: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get weights and biases from the Dense layer\n",
        "weights, biases = model.layers[0].get_weights()\n",
        "\n",
        "print(\"\\nWeights:\")\n",
        "print(weights)\n",
        "\n",
        "print(\"\\nBias:\")\n",
        "print(biases)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGzOHxckxOl6",
        "outputId": "fcacd0c7-0370-4e37-a2f1-a5ac9b589e7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Weights:\n",
            "[[7.423303 ]\n",
            " [7.4233027]]\n",
            "\n",
            "Bias:\n",
            "[-11.307543]\n"
          ]
        }
      ]
    }
  ]
}